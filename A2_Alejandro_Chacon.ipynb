{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alechacon99/INST767-SP2023/blob/main/A2_Alejandro_Chacon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "\n",
        "In this assignment, you are asked to calculate the word co-occurrence of the Shakespeare collection and check its correctness using a non-Hadoop approach. Specifically, we ask you to implement the **pairs** design pattern.\n",
        "\n",
        "We have prepared some test cases to help you verify the correctness of your code. Correctly implemented mapper and reducer functions should produce results as indicated in the test cells:\n",
        "1. How many pairs of words cooccurred at least 10 times?\n",
        "2. How many times does the word 'macbeth' and 'lady' cooccur?\n",
        "3. Which pair of words have the largest cooccurrence?\n",
        "\n",
        "We have prepared the code to help you check your answer. However, we also ask you to **calculate the word co-occurrence without using Hadoop**. Could you answer the previous three questions with a non-Hadoop solution?\n",
        "\n",
        "Some specific requirements are as follows:\n",
        "1. Your mapper should output cooccurrence pairs as `(word1, word2)\\t[NUM]`. This is because the Hadoop use `\\t` to split key and values.\n",
        "2. You should output *unique* cooccurrence of different words for each line. That is, a line containing four word \"A A B C\" should yield 6 cooccurrence pairs -- (A,B),(B,A),(A,C),(C,A),(B,C),(C,B).\n",
        "3. We are only interested in pairs of words that cooccur at least 10 times.\n",
        "\n",
        "The CoLab notebook is organized as follows:\n",
        "1. In the first section **Installing Hadoop**, we provide you the basic code to install Hadoop. This is the same code we used during the classroom demo.\n",
        "We made a couple of changes compared with Week 1:\n",
        "2. In the second section **Shakespeare Word Count**, we download the dataset and provide you the basic mapper and reducer for the word count example demonstrated in the classroom. This allows you to verify the successful installation and basic running of hadoop.\n",
        "3. In **Word Co-occurrence as Pairs**, we ask you to complete the mapper and reducer function and run them with the Hadoop command.\n",
        "4. In **Checking your answer**, we prepared codes to check your results.\n",
        "5. Finally, in **Calculate Co-occurrence without Hadoop?**, we ask you to write your own code to calculate the cooccurrence and answer the same three questions.\n",
        "\n",
        "**Grades:** This assignment accounts for 10 points. \n",
        "- 4 points for implementing the mapper and reducer correctly.\n",
        "- 3 points for using the Hadoop output to answer the three questions correctly. - 3 points for a seperate implementation without using non-Hadoop.\n",
        "\n",
        "**Submission:** Please download your notebook and submit them to ELMS. We will make a minimal effort to fix trivial issues with your code and deduct points, but will not spend time debugging your code. It is your responsibility to make sure your code runs in CoLab. If anything is unclear, it is your responsibility to seek clarification."
      ],
      "metadata": {
        "id": "oSj4TGS0xBRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Hadoop"
      ],
      "metadata": {
        "id": "jmVdE3ic3Caz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtGsA5LknFpL",
        "outputId": "a361435c-95f6-4baa-c8e2-e04682aaaaa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-17 17:35:53--  https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 695457782 (663M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.4.tar.gz’\n",
            "\n",
            "hadoop-3.3.4.tar.gz 100%[===================>] 663.24M  31.1MB/s    in 22s     \n",
            "\n",
            "2023-02-17 17:36:15 (30.4 MB/s) - ‘hadoop-3.3.4.tar.gz’ saved [695457782/695457782]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf hadoop-3.3.4.tar.gz"
      ],
      "metadata": {
        "id": "zJLL4A78nr9t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vEJCCSwXcvJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r hadoop-3.3.4/ /usr/local/"
      ],
      "metadata": {
        "id": "n_1y7L5mnwIT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMxznGpYn9UP",
        "outputId": "ed7853da-230e-41f9-ce0f-3c5d1b32a252"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing os module\n",
        "import os\n",
        "#Creating environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\""
      ],
      "metadata": {
        "id": "6p-M-aiAobw_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/input\n",
        "!cp /usr/local/hadoop-3.3.4/etc/hadoop/*.xml ~/input"
      ],
      "metadata": {
        "id": "1hHJ2iGpo4O2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Word Count\n",
        "\n",
        "We made a couple of changes compared with Week 1:\n",
        "1. We write a new tokenizer for Mapper, which removes unnecessary punctuations.\n",
        "2. We filter out rare words (words that appears less than 10 times) in reducer."
      ],
      "metadata": {
        "id": "BaTVnq_z2-FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Shakespear collection\n",
        "!wget https://www.gutenberg.org/cache/epub/100/pg100.txt"
      ],
      "metadata": {
        "id": "P-_vD8IX6ut9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4fe5413-92a9-459f-a4e7-690bd2cd890e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-17 23:01:50--  https://www.gutenberg.org/cache/epub/100/pg100.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5732001 (5.5M) [text/plain]\n",
            "Saving to: ‘pg100.txt’\n",
            "\n",
            "pg100.txt           100%[===================>]   5.47M  10.7MB/s    in 0.5s    \n",
            "\n",
            "2023-02-17 23:01:51 (10.7 MB/s) - ‘pg100.txt’ saved [5732001/5732001]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv pg100.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "23XBl82Myv_4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapperpy = '''#!/usr/bin/env python\n",
        "\"\"\"mapper.py\"\"\"\n",
        "import sys\n",
        "import re\n",
        "# input comes from STDIN (standard input)\n",
        "\n",
        "PAT = re.compile(\"(^[^a-z]+|[^a-z]+$)\")\n",
        "\n",
        "def tokenize(line):\n",
        "    #tokenize the string, remove unnecessary characters\n",
        "    tokens = [re.sub(PAT, \"\", t.lower()) for t in line.split()]\n",
        "    #returning non-empty strings as tokenization result\n",
        "    return list(filter(lambda x:len(x)>0, tokens))\n",
        "\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # tokenize the line into words\n",
        "    tokens = tokenize(line)\n",
        "    # increase counters\n",
        "    for t in tokens:\n",
        "        # write the results to STDOUT (standard output);\n",
        "        # what we output here will be the input for the\n",
        "        # Reduce step, i.e. the input for reducer.py\n",
        "        #\n",
        "        # tab-delimited; the trivial word count is 1\n",
        "        print('%s\\t%s' % (t, 1))\n",
        "'''\n",
        "\n",
        "with open(\"mapper.py\", \"w\") as fmap:\n",
        "  fmap.write(mapperpy)\n",
        "\n",
        "!chmod +x mapper.py"
      ],
      "metadata": {
        "id": "zczkLopy2zaB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reducerpy = '''#!/usr/bin/env python\n",
        "\"\"\"reducer.py\"\"\"\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # parse the input we got from mapper.py\n",
        "    word, count = line.split('\\t', 1)\n",
        "    # convert count (currently a string) to int\n",
        "    try:\n",
        "        count = int(count)\n",
        "    except ValueError:\n",
        "        # count was not a number, so silently\n",
        "        # ignore/discard this line\n",
        "        continue\n",
        "    # this IF-switch only works because Hadoop sorts map output\n",
        "    # by key (here: word) before it is passed to the reducer\n",
        "    if current_word == word:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_word:\n",
        "            # write result to STDOUT\n",
        "            if current_count >= 10:\n",
        "                print('%s\\t%s' % (current_word, current_count))\n",
        "        current_count = count\n",
        "        current_word = word\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "    if current_count >= 10:\n",
        "        print('%s\\t%s' % (current_word, current_count))\n",
        "'''\n",
        "\n",
        "with open(\"reducer.py\", \"w\") as freduce:\n",
        "  freduce.write(reducerpy)\n",
        "\n",
        "!chmod +x reducer.py"
      ],
      "metadata": {
        "id": "SlyhaQpR6sER"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/output\n",
        "\n",
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar /usr/local/hadoop-3.3.4/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar \\\n",
        "  -input /content/shakespeare.txt \\\n",
        "  -output /content/output \\\n",
        "  -numReduceTasks 10 \\\n",
        "  -mapper \"python /content/mapper.py\" \\\n",
        "  -reducer \"python /content/reducer.py\""
      ],
      "metadata": {
        "id": "cvBbnNnh6z_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Co-occurrence as Pairs\n",
        "Below we provide a skeleton mapper and reducer for you. Please complete the mapper and reducer functions to calculate the word co-occurrence as pairs. "
      ],
      "metadata": {
        "id": "_U9VBtd42MUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_mapper = '''#!/usr/bin/env python\n",
        "\"\"\"mapper.py\"\"\"\n",
        "import sys\n",
        "import re\n",
        "# input comes from STDIN (standard input)\n",
        "\n",
        "PAT = re.compile(\"(^[^a-z]+|[^a-z]+$)\")\n",
        "\n",
        "def tokenize(line):\n",
        "    #tokenize the string, remove unnecessary characters\n",
        "    tokens = [re.sub(PAT, \"\", t.lower()) for t in line.split()]\n",
        "    #returning non-empty strings as tokenization result\n",
        "    return list(filter(lambda x:len(x)>0, tokens))\n",
        "\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # tokenize the line into words\n",
        "    tokens = tokenize(line)\n",
        "    # increase counters\n",
        "    t_counter = []\n",
        "    for t in tokens:\n",
        "      #next for loop needs to iterate through every token in tokens except t\n",
        "      if t in t_counter:\n",
        "        continue\n",
        "      else:\n",
        "        t_counter.append(t)\n",
        "        v_counter = []\n",
        "        for v in tokens:\n",
        "          if v == t:\n",
        "            continue\n",
        "          elif v in v_counter:\n",
        "            continue\n",
        "        # write the results to STDOUT (standard output);\n",
        "        # what we output here will be the input for the\n",
        "        # Reduce step, i.e. the input for reducer.py\n",
        "        #\n",
        "        # tab-delimited; the trivial word count is 1\n",
        "          else:\n",
        "            v_counter.append(v)\n",
        "            print('%s\\t%s' % (('('+t+', '+v+')'), 1))\n",
        "\n",
        "'''\n",
        "\n",
        "with open(\"pairs_mapper.py\", \"w\") as fmap:\n",
        "  fmap.write(pairs_mapper)\n",
        "\n",
        "!chmod +x pairs_mapper.py"
      ],
      "metadata": {
        "id": "bVIuNjoV1j7o"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_reducer = '''#!/usr/bin/env python\n",
        "\"\"\"reducer.py\"\"\"\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_pair = None\n",
        "current_count = 0\n",
        "pair = None\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # parse the input we got from mapper.py\n",
        "    pair, count = line.split('\\t', 1)\n",
        "    # convert count (currently a string) to int\n",
        "    try:\n",
        "        count = int(count)\n",
        "    except ValueError:\n",
        "        # count was not a number, so silently\n",
        "        # ignore/discard this line\n",
        "        continue\n",
        "    # this IF-switch only works because Hadoop sorts map output\n",
        "    # by key (here: pair) before it is passed to the reducer\n",
        "    if current_pair == pair:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_pair:\n",
        "            # write result to STDOUT\n",
        "            if current_count >= 10:\n",
        "              print('%s\\t%s' % (current_pair, current_count))\n",
        "        current_count = count\n",
        "        current_pair = pair\n",
        "# do not forget to output the last pair if needed!\n",
        "if current_pair == pair:\n",
        "  if current_count >= 10:\n",
        "    print('%s\\t%s' % (current_pair, current_count))\n",
        "\n",
        "'''\n",
        "\n",
        "with open(\"pairs_reducer.py\", \"w\") as freduce:\n",
        "  freduce.write(pairs_reducer)\n",
        "\n",
        "!chmod +x pairs_reducer.py"
      ],
      "metadata": {
        "id": "lL8WNjkByjgH"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/pairs_output\n",
        "\n",
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar /usr/local/hadoop-3.3.4/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar \\\n",
        "  -input /content/shakespeare.txt \\\n",
        "  -output /content/pairs_output \\\n",
        "  -numReduceTasks 10 \\\n",
        "  -mapper \"python /content/pairs_mapper.py\" \\\n",
        "  -reducer \"python /content/pairs_reducer.py\""
      ],
      "metadata": {
        "id": "TfQAxfDxzI0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking your answer"
      ],
      "metadata": {
        "id": "EVgFdXe52hR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code checks how many co-occurrence pairs there are, that is, \n",
        "# the total number of lines in the output files. \n",
        "# The correct answer should be 78498\n",
        "!cat /content/pairs_output/* | wc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V81b2iosz_Dl",
        "outputId": "d80e6be8-fb80-4b0d-82bb-fb53b6039992"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  78498  235494 1248414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code checks how many times the word 'macbeth' co-occurred with\n",
        "# the word 'lady'.\n",
        "# The correct answer should be 73\n",
        "!grep \"(macbeth, lady)\" /content/pairs_output/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cronvq90SmP",
        "outputId": "1b2b6e37-bf1b-4dcc-b354-db154a9cfb64"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pairs_output/part-00006:(macbeth, lady)\t73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code checks which pair of words co-occurred the most times.\n",
        "# The correct answer should be 'the' and 'of'.\n",
        "!cat /content/pairs_output/* | sort -k 3 -g -r |head -2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxVxPgDA0qnq",
        "outputId": "fea63b14-ef03-4dc8-a759-eb68247079c5"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(the, of)\t7914\n",
            "(of, the)\t7914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Co-occurrence without Hadoop?\n",
        "\n",
        "Below, please redo the co-occurrence calculation without using Hadoop. You may use whatever method that you are familiar with. Do they "
      ],
      "metadata": {
        "id": "QvxKqw883Mvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/shakespeare.txt', 'r')\n"
      ],
      "metadata": {
        "id": "I36UHEKf0xP8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "PAT = re.compile(\"(^[^a-z]+|[^a-z]+$)\")\n",
        "\n",
        "def tokenize(line):\n",
        "    #tokenize the string, remove unnecessary characters\n",
        "    tokens = [re.sub(PAT, \"\", t.lower()) for t in line.split()]\n",
        "    #returning non-empty strings as tokenization result\n",
        "    return list(filter(lambda x:len(x)>0, tokens))\n",
        "\n",
        "mapper_output = []\n",
        "\n",
        "for line in file:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # tokenize the line into words\n",
        "    tokens = tokenize(line)\n",
        "    # increase counters\n",
        "    t_counter = []\n",
        "    for t in tokens:\n",
        "      #next for loop needs to iterate through every token in tokens except t\n",
        "      if t in t_counter:\n",
        "        continue\n",
        "      else:\n",
        "        t_counter.append(t)\n",
        "        v_counter = []\n",
        "        for v in tokens:\n",
        "          if v == t:\n",
        "            continue\n",
        "          elif v in v_counter:\n",
        "            continue\n",
        "          else:\n",
        "            v_counter.append(v)\n",
        "            mapper_output.append('('+t+', '+v+')')"
      ],
      "metadata": {
        "id": "tI4gJuuPDJes"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mapper_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY7NghUwpWQS",
        "outputId": "b1f9b5c7-42d4-4070-fc5f-68424a71bb85"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6672860"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reducer_ram = {}\n",
        "reducer_output = {}\n",
        "\n",
        "# input comes from STDIN\n",
        "for u in mapper_output:\n",
        "    if u in reducer_ram:\n",
        "        reducer_ram[u] += 1\n",
        "    else:\n",
        "        reducer_ram[u] = 1\n",
        "\n",
        "for u in reducer_ram:\n",
        "  if reducer_ram[u] >= 10:\n",
        "    reducer_output[u] = reducer_ram[u]\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "id": "8a0eXBzmDJ-s"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(reducer_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIMllqctDyv6",
        "outputId": "dbef5947-b1f1-4c90-fed5-a18f075a6ab0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78498"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reducer_output[\"(macbeth, lady)\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSR5JEAUozJ2",
        "outputId": "03b5c28c-a5a9-428a-bb57-5476f0fdf6ad"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "max = dict(sorted(reducer_output.items(), key=itemgetter(1), reverse=True)[:2])\n",
        "max"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf3FLJW_tpIQ",
        "outputId": "a1af3811-6a9b-4768-92ed-3edce6fdf0cb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(the, of)': 7914, '(of, the)': 7914}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}